{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"다음 주 화요일에 연세대학교 대우관 별관에서 뵙겠습니다.\\n\n",
    "연락처를 남겨주세요\\n\n",
    "다른 교수님들에게 의견을 들어봐야 합니다.\\n\n",
    "학생들의 의견을 알아보겠습니다.\\n\n",
    "이메일 주소가 어떻게 되세요?\\n\n",
    "감사합니다.\\n\n",
    "잘 모르겠는데요\\n\n",
    "주임교수님과 상의하고 연락드리겠습니다.\\n\n",
    "다음 주 화요일에 연세대학교 대우관 별관에서 뵙겠습니다.\\n\n",
    "연락처를 남겨주세요\\n\n",
    "다른 교수님들에게 의견을 들어봐야 합니다.\\n\n",
    "학생들의 의견을 알아보겠습니다.\\n\n",
    "이메일 주소가 어떻게 되세요?\\n\n",
    "감사합니다.\\n\n",
    "잘 모르겠는데요\\n\n",
    "주임교수님과 상의하고 연락드리겠습니다.\\n\n",
    "다음 주 화요일에 연세대학교 대우관 별관에서 뵙겠습니다.\\n\n",
    "연락처를 남겨주세요\\n\n",
    "다른 교수님들에게 의견을 들어봐야 합니다.\\n\n",
    "학생들의 의견을 알아보겠습니다.\\n\n",
    "이메일 주소가 어떻게 되세요?\\n\n",
    "감사합니다.\\n\n",
    "잘 모르겠는데요\\n\n",
    "주임교수님과 상의하고 연락드리겠습니다.\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.text import Tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts([text])\n",
    "encoded = t.texts_to_sequences([text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 27\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(t.word_index) + 1\n",
    "# 케라스 토크나이저의 정수 인코딩은 인덱스가 1부터 시작하지만,\n",
    "# 케라스 원-핫 인코딩에서 배열의 인덱스가 0부터 시작하기 때문에\n",
    "# 배열의 크기를 실제 단어 집합의 크기보다 +1로 생성해야하므로 미리 +1 선언 \n",
    "print('단어 집합의 크기 : %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'의견을': 1, '다음': 2, '주': 3, '화요일에': 4, '연세대학교': 5, '대우관': 6, '별관에서': 7, '뵙겠습니다': 8, '연락처를': 9, '남겨주세요': 10, '다른': 11, '교수님들에게': 12, '들어봐야': 13, '합니다': 14, '학생들의': 15, '알아보겠습니다': 16, '이메일': 17, '주소가': 18, '어떻게': 19, '되세요': 20, '감사합니다': 21, '잘': 22, '모르겠는데요': 23, '주임교수님과': 24, '상의하고': 25, '연락드리겠습니다': 26}\n"
     ]
    }
   ],
   "source": [
    "print(t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 개수: 57\n"
     ]
    }
   ],
   "source": [
    "sequences = list()\n",
    "for line in text.split('\\n'): # Wn을 기준으로 문장 토큰화\n",
    "    encoded = t.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence = encoded[:i+1]\n",
    "        sequences.append(sequence)\n",
    "\n",
    "print('훈련 데이터의 개수: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3], [2, 3, 4], [2, 3, 4, 5], [2, 3, 4, 5, 6], [2, 3, 4, 5, 6, 7], [2, 3, 4, 5, 6, 7, 8], [9, 10], [11, 12], [11, 12, 1], [11, 12, 1, 13], [11, 12, 1, 13, 14], [15, 1], [15, 1, 16], [17, 18], [17, 18, 19], [17, 18, 19, 20], [22, 23], [24, 25], [24, 25, 26], [2, 3], [2, 3, 4], [2, 3, 4, 5], [2, 3, 4, 5, 6], [2, 3, 4, 5, 6, 7], [2, 3, 4, 5, 6, 7, 8], [9, 10], [11, 12], [11, 12, 1], [11, 12, 1, 13], [11, 12, 1, 13, 14], [15, 1], [15, 1, 16], [17, 18], [17, 18, 19], [17, 18, 19, 20], [22, 23], [24, 25], [24, 25, 26], [2, 3], [2, 3, 4], [2, 3, 4, 5], [2, 3, 4, 5, 6], [2, 3, 4, 5, 6, 7], [2, 3, 4, 5, 6, 7, 8], [9, 10], [11, 12], [11, 12, 1], [11, 12, 1, 13], [11, 12, 1, 13, 14], [15, 1], [15, 1, 16], [17, 18], [17, 18, 19], [17, 18, 19, 20], [22, 23], [24, 25], [24, 25, 26]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = max(len(l) for l in sequences)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sequences = np.array(sequences)\n",
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]\n",
    "# 리스트의 마지막 열을 제외하고 저장한 것은 X\n",
    "# 리스트의 마지막 열만 저장한 것은 y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  2]\n",
      " [ 0  0  0  0  2  3]\n",
      " [ 0  0  0  2  3  4]\n",
      " [ 0  0  2  3  4  5]\n",
      " [ 0  2  3  4  5  6]\n",
      " [ 2  3  4  5  6  7]\n",
      " [ 0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0 11]\n",
      " [ 0  0  0  0 11 12]\n",
      " [ 0  0  0 11 12  1]\n",
      " [ 0  0 11 12  1 13]\n",
      " [ 0  0  0  0  0 15]\n",
      " [ 0  0  0  0 15  1]\n",
      " [ 0  0  0  0  0 17]\n",
      " [ 0  0  0  0 17 18]\n",
      " [ 0  0  0 17 18 19]\n",
      " [ 0  0  0  0  0 22]\n",
      " [ 0  0  0  0  0 24]\n",
      " [ 0  0  0  0 24 25]\n",
      " [ 0  0  0  0  0  2]\n",
      " [ 0  0  0  0  2  3]\n",
      " [ 0  0  0  2  3  4]\n",
      " [ 0  0  2  3  4  5]\n",
      " [ 0  2  3  4  5  6]\n",
      " [ 2  3  4  5  6  7]\n",
      " [ 0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0 11]\n",
      " [ 0  0  0  0 11 12]\n",
      " [ 0  0  0 11 12  1]\n",
      " [ 0  0 11 12  1 13]\n",
      " [ 0  0  0  0  0 15]\n",
      " [ 0  0  0  0 15  1]\n",
      " [ 0  0  0  0  0 17]\n",
      " [ 0  0  0  0 17 18]\n",
      " [ 0  0  0 17 18 19]\n",
      " [ 0  0  0  0  0 22]\n",
      " [ 0  0  0  0  0 24]\n",
      " [ 0  0  0  0 24 25]\n",
      " [ 0  0  0  0  0  2]\n",
      " [ 0  0  0  0  2  3]\n",
      " [ 0  0  0  2  3  4]\n",
      " [ 0  0  2  3  4  5]\n",
      " [ 0  2  3  4  5  6]\n",
      " [ 2  3  4  5  6  7]\n",
      " [ 0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0 11]\n",
      " [ 0  0  0  0 11 12]\n",
      " [ 0  0  0 11 12  1]\n",
      " [ 0  0 11 12  1 13]\n",
      " [ 0  0  0  0  0 15]\n",
      " [ 0  0  0  0 15  1]\n",
      " [ 0  0  0  0  0 17]\n",
      " [ 0  0  0  0 17 18]\n",
      " [ 0  0  0 17 18 19]\n",
      " [ 0  0  0  0  0 22]\n",
      " [ 0  0  0  0  0 24]\n",
      " [ 0  0  0  0 24 25]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  4  5  6  7  8 10 12  1 13 14  1 16 18 19 20 23 25 26  3  4  5  6  7\n",
      "  8 10 12  1 13 14  1 16 18 19 20 23 25 26  3  4  5  6  7  8 10 12  1 13\n",
      " 14  1 16 18 19 20 23 25 26]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Dense, SimpleRNN, Dropout, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_FOLDER = './model/'\n",
    "import os\n",
    "if not os.path.exists(MODEL_SAVE_FOLDER):\n",
    "    os.mkdir(MODEL_SAVE_FOLDER)\n",
    "\n",
    "path = MODEL_SAVE_FOLDER + '{epoch:02d}-{accuracy:.4f}.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 아래 셀에 체크포인트 콜백을 정의하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.ModelCheckpoint at 0x1c5dd9ad710>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=path, monitor='val_accuracy', verbose=1, save_best_only=True)\n",
    "checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 아래 셀에 얼리스탑핑 콜백을 정의하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.EarlyStopping at 0x1c5dd979898>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "## 얼리스탑핑 콜백을 정의하세요.\n",
    "early_stopping = EarlyStopping(monitor='loss', min_delta=0.01, patience=10)\n",
    "early_stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28 samples, validate on 29 samples\n",
      "Epoch 1/2000\n",
      " - 0s - loss: 3.2966 - accuracy: 0.0000e+00 - val_loss: 3.2896 - val_accuracy: 0.1034\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.10345, saving model to ./model/01-0.0000.hdf5\n",
      "Epoch 2/2000\n",
      " - 0s - loss: 3.2889 - accuracy: 0.1071 - val_loss: 3.2844 - val_accuracy: 0.1379\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.10345 to 0.13793, saving model to ./model/02-0.1071.hdf5\n",
      "Epoch 3/2000\n",
      " - 0s - loss: 3.2811 - accuracy: 0.1786 - val_loss: 3.2794 - val_accuracy: 0.1379\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.13793\n",
      "Epoch 4/2000\n",
      " - 0s - loss: 3.2737 - accuracy: 0.1786 - val_loss: 3.2739 - val_accuracy: 0.1034\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.13793\n",
      "Epoch 5/2000\n",
      " - 0s - loss: 3.2657 - accuracy: 0.1071 - val_loss: 3.2676 - val_accuracy: 0.1034\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.13793\n",
      "Epoch 6/2000\n",
      " - 0s - loss: 3.2565 - accuracy: 0.1071 - val_loss: 3.2609 - val_accuracy: 0.1034\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.13793\n",
      "Epoch 7/2000\n",
      " - 0s - loss: 3.2467 - accuracy: 0.1071 - val_loss: 3.2532 - val_accuracy: 0.1379\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.13793\n",
      "Epoch 8/2000\n",
      " - 0s - loss: 3.2356 - accuracy: 0.1786 - val_loss: 3.2447 - val_accuracy: 0.1379\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.13793\n",
      "Epoch 9/2000\n",
      " - 0s - loss: 3.2232 - accuracy: 0.1786 - val_loss: 3.2352 - val_accuracy: 0.1379\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.13793\n",
      "Epoch 10/2000\n",
      " - 0s - loss: 3.2094 - accuracy: 0.1786 - val_loss: 3.2243 - val_accuracy: 0.1379\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.13793\n",
      "Epoch 11/2000\n",
      " - 0s - loss: 3.1935 - accuracy: 0.1786 - val_loss: 3.2120 - val_accuracy: 0.1034\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.13793\n",
      "Epoch 12/2000\n",
      " - 0s - loss: 3.1754 - accuracy: 0.1071 - val_loss: 3.1980 - val_accuracy: 0.1034\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.13793\n",
      "Epoch 13/2000\n",
      " - 0s - loss: 3.1546 - accuracy: 0.1071 - val_loss: 3.1821 - val_accuracy: 0.1034\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.13793\n",
      "Epoch 14/2000\n",
      " - 0s - loss: 3.1307 - accuracy: 0.1071 - val_loss: 3.1642 - val_accuracy: 0.1034\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.13793\n",
      "Epoch 15/2000\n",
      " - 0s - loss: 3.1031 - accuracy: 0.1071 - val_loss: 3.1445 - val_accuracy: 0.1034\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.13793\n",
      "Epoch 16/2000\n",
      " - 0s - loss: 3.0717 - accuracy: 0.1071 - val_loss: 3.1232 - val_accuracy: 0.1034\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.13793\n",
      "Epoch 17/2000\n",
      " - 0s - loss: 3.0363 - accuracy: 0.1071 - val_loss: 3.1014 - val_accuracy: 0.1034\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.13793\n",
      "Epoch 18/2000\n",
      " - 0s - loss: 2.9970 - accuracy: 0.1071 - val_loss: 3.0809 - val_accuracy: 0.1034\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.13793\n",
      "Epoch 19/2000\n",
      " - 0s - loss: 2.9544 - accuracy: 0.1071 - val_loss: 3.0656 - val_accuracy: 0.1034\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.13793\n",
      "Epoch 20/2000\n",
      " - 0s - loss: 2.9115 - accuracy: 0.1071 - val_loss: 3.0611 - val_accuracy: 0.1034\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.13793\n",
      "Epoch 21/2000\n",
      " - 0s - loss: 2.8733 - accuracy: 0.1071 - val_loss: 3.0727 - val_accuracy: 0.1034\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.13793\n",
      "Epoch 22/2000\n",
      " - 0s - loss: 2.8445 - accuracy: 0.1071 - val_loss: 3.0959 - val_accuracy: 0.1034\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.13793\n",
      "Epoch 23/2000\n",
      " - 0s - loss: 2.8265 - accuracy: 0.1071 - val_loss: 3.1141 - val_accuracy: 0.1034\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.13793\n",
      "Epoch 24/2000\n",
      " - 0s - loss: 2.8099 - accuracy: 0.1071 - val_loss: 3.1123 - val_accuracy: 0.1034\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.13793\n",
      "Epoch 25/2000\n",
      " - 0s - loss: 2.7861 - accuracy: 0.1071 - val_loss: 3.0884 - val_accuracy: 0.1034\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.13793\n",
      "Epoch 26/2000\n",
      " - 0s - loss: 2.7526 - accuracy: 0.1071 - val_loss: 3.0492 - val_accuracy: 0.1034\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.13793\n",
      "Epoch 27/2000\n",
      " - 0s - loss: 2.7141 - accuracy: 0.1071 - val_loss: 3.0025 - val_accuracy: 0.1034\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.13793\n",
      "Epoch 28/2000\n",
      " - 0s - loss: 2.6750 - accuracy: 0.1071 - val_loss: 2.9548 - val_accuracy: 0.1379\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.13793\n",
      "Epoch 29/2000\n",
      " - 0s - loss: 2.6386 - accuracy: 0.1786 - val_loss: 2.9090 - val_accuracy: 0.1379\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.13793\n",
      "Epoch 30/2000\n",
      " - 0s - loss: 2.6056 - accuracy: 0.1786 - val_loss: 2.8658 - val_accuracy: 0.1724\n",
      "\n",
      "Epoch 00030: val_accuracy improved from 0.13793 to 0.17241, saving model to ./model/30-0.1786.hdf5\n",
      "Epoch 31/2000\n",
      " - 0s - loss: 2.5748 - accuracy: 0.2500 - val_loss: 2.8239 - val_accuracy: 0.2069\n",
      "\n",
      "Epoch 00031: val_accuracy improved from 0.17241 to 0.20690, saving model to ./model/31-0.2500.hdf5\n",
      "Epoch 32/2000\n",
      " - 0s - loss: 2.5440 - accuracy: 0.3214 - val_loss: 2.7819 - val_accuracy: 0.2414\n",
      "\n",
      "Epoch 00032: val_accuracy improved from 0.20690 to 0.24138, saving model to ./model/32-0.3214.hdf5\n",
      "Epoch 33/2000\n",
      " - 0s - loss: 2.5113 - accuracy: 0.3929 - val_loss: 2.7389 - val_accuracy: 0.2414\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.24138\n",
      "Epoch 34/2000\n",
      " - 0s - loss: 2.4753 - accuracy: 0.3929 - val_loss: 2.6948 - val_accuracy: 0.2414\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.24138\n",
      "Epoch 35/2000\n",
      " - 0s - loss: 2.4352 - accuracy: 0.3929 - val_loss: 2.6504 - val_accuracy: 0.2414\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.24138\n",
      "Epoch 36/2000\n",
      " - 0s - loss: 2.3914 - accuracy: 0.3929 - val_loss: 2.6073 - val_accuracy: 0.2414\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.24138\n",
      "Epoch 37/2000\n",
      " - 0s - loss: 2.3447 - accuracy: 0.3929 - val_loss: 2.5674 - val_accuracy: 0.2414\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.24138\n",
      "Epoch 38/2000\n",
      " - 0s - loss: 2.2962 - accuracy: 0.3929 - val_loss: 2.5323 - val_accuracy: 0.1724\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.24138\n",
      "Epoch 39/2000\n",
      " - 0s - loss: 2.2473 - accuracy: 0.2500 - val_loss: 2.5024 - val_accuracy: 0.1724\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.24138\n",
      "Epoch 40/2000\n",
      " - 0s - loss: 2.1989 - accuracy: 0.2500 - val_loss: 2.4777 - val_accuracy: 0.1724\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.24138\n",
      "Epoch 41/2000\n",
      " - 0s - loss: 2.1520 - accuracy: 0.2500 - val_loss: 2.4571 - val_accuracy: 0.1724\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.24138\n",
      "Epoch 42/2000\n",
      " - 0s - loss: 2.1070 - accuracy: 0.2500 - val_loss: 2.4387 - val_accuracy: 0.1724\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.24138\n",
      "Epoch 43/2000\n",
      " - 0s - loss: 2.0650 - accuracy: 0.2500 - val_loss: 2.4188 - val_accuracy: 0.2069\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.24138\n",
      "Epoch 44/2000\n",
      " - 0s - loss: 2.0247 - accuracy: 0.3214 - val_loss: 2.3937 - val_accuracy: 0.2069\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.24138\n",
      "Epoch 45/2000\n",
      " - 0s - loss: 1.9840 - accuracy: 0.3214 - val_loss: 2.3637 - val_accuracy: 0.2069\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.24138\n",
      "Epoch 46/2000\n",
      " - 0s - loss: 1.9434 - accuracy: 0.3214 - val_loss: 2.3300 - val_accuracy: 0.2414\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.24138\n",
      "Epoch 47/2000\n",
      " - 0s - loss: 1.9042 - accuracy: 0.3929 - val_loss: 2.2932 - val_accuracy: 0.3448\n",
      "\n",
      "Epoch 00047: val_accuracy improved from 0.24138 to 0.34483, saving model to ./model/47-0.3929.hdf5\n",
      "Epoch 48/2000\n",
      " - 0s - loss: 1.8665 - accuracy: 0.6071 - val_loss: 2.2532 - val_accuracy: 0.3448\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.34483\n",
      "Epoch 49/2000\n",
      " - 0s - loss: 1.8286 - accuracy: 0.6071 - val_loss: 2.2135 - val_accuracy: 0.3103\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.34483\n",
      "Epoch 50/2000\n",
      " - 0s - loss: 1.7926 - accuracy: 0.5357 - val_loss: 2.1734 - val_accuracy: 0.3103\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.34483\n",
      "Epoch 51/2000\n",
      " - 0s - loss: 1.7563 - accuracy: 0.5357 - val_loss: 2.1335 - val_accuracy: 0.3103\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.34483\n",
      "Epoch 52/2000\n",
      " - 0s - loss: 1.7194 - accuracy: 0.5357 - val_loss: 2.0961 - val_accuracy: 0.3103\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.34483\n",
      "Epoch 53/2000\n",
      " - 0s - loss: 1.6834 - accuracy: 0.5357 - val_loss: 2.0601 - val_accuracy: 0.3103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.34483\n",
      "Epoch 54/2000\n",
      " - 0s - loss: 1.6465 - accuracy: 0.5357 - val_loss: 2.0265 - val_accuracy: 0.3103\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.34483\n",
      "Epoch 55/2000\n",
      " - 0s - loss: 1.6103 - accuracy: 0.5357 - val_loss: 1.9912 - val_accuracy: 0.3103\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.34483\n",
      "Epoch 56/2000\n",
      " - 0s - loss: 1.5736 - accuracy: 0.5357 - val_loss: 1.9534 - val_accuracy: 0.4483\n",
      "\n",
      "Epoch 00056: val_accuracy improved from 0.34483 to 0.44828, saving model to ./model/56-0.5357.hdf5\n",
      "Epoch 57/2000\n",
      " - 0s - loss: 1.5381 - accuracy: 0.7143 - val_loss: 1.9104 - val_accuracy: 0.4138\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.44828\n",
      "Epoch 58/2000\n",
      " - 0s - loss: 1.5019 - accuracy: 0.6429 - val_loss: 1.8633 - val_accuracy: 0.4138\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.44828\n",
      "Epoch 59/2000\n",
      " - 0s - loss: 1.4668 - accuracy: 0.6429 - val_loss: 1.8159 - val_accuracy: 0.5172\n",
      "\n",
      "Epoch 00059: val_accuracy improved from 0.44828 to 0.51724, saving model to ./model/59-0.6429.hdf5\n",
      "Epoch 60/2000\n",
      " - 0s - loss: 1.4327 - accuracy: 0.7500 - val_loss: 1.7749 - val_accuracy: 0.4483\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.51724\n",
      "Epoch 61/2000\n",
      " - 0s - loss: 1.3992 - accuracy: 0.6071 - val_loss: 1.7415 - val_accuracy: 0.5172\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.51724\n",
      "Epoch 62/2000\n",
      " - 0s - loss: 1.3665 - accuracy: 0.7500 - val_loss: 1.7128 - val_accuracy: 0.4483\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.51724\n",
      "Epoch 63/2000\n",
      " - 0s - loss: 1.3346 - accuracy: 0.6071 - val_loss: 1.6910 - val_accuracy: 0.5172\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.51724\n",
      "Epoch 64/2000\n",
      " - 0s - loss: 1.3063 - accuracy: 0.7500 - val_loss: 1.6566 - val_accuracy: 0.5517\n",
      "\n",
      "Epoch 00064: val_accuracy improved from 0.51724 to 0.55172, saving model to ./model/64-0.7500.hdf5\n",
      "Epoch 65/2000\n",
      " - 0s - loss: 1.2740 - accuracy: 0.7143 - val_loss: 1.6196 - val_accuracy: 0.5172\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.55172\n",
      "Epoch 66/2000\n",
      " - 0s - loss: 1.2403 - accuracy: 0.7500 - val_loss: 1.5741 - val_accuracy: 0.5862\n",
      "\n",
      "Epoch 00066: val_accuracy improved from 0.55172 to 0.58621, saving model to ./model/66-0.7500.hdf5\n",
      "Epoch 67/2000\n",
      " - 0s - loss: 1.2067 - accuracy: 0.7857 - val_loss: 1.5337 - val_accuracy: 0.6552\n",
      "\n",
      "Epoch 00067: val_accuracy improved from 0.58621 to 0.65517, saving model to ./model/67-0.7857.hdf5\n",
      "Epoch 68/2000\n",
      " - 0s - loss: 1.1800 - accuracy: 0.8214 - val_loss: 1.5073 - val_accuracy: 0.5862\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.65517\n",
      "Epoch 69/2000\n",
      " - 0s - loss: 1.1595 - accuracy: 0.7857 - val_loss: 1.4674 - val_accuracy: 0.6207\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.65517\n",
      "Epoch 70/2000\n",
      " - 0s - loss: 1.1322 - accuracy: 0.7500 - val_loss: 1.4387 - val_accuracy: 0.7241\n",
      "\n",
      "Epoch 00070: val_accuracy improved from 0.65517 to 0.72414, saving model to ./model/70-0.7500.hdf5\n",
      "Epoch 71/2000\n",
      " - 0s - loss: 1.1014 - accuracy: 0.8571 - val_loss: 1.3993 - val_accuracy: 0.7241\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.72414\n",
      "Epoch 72/2000\n",
      " - 0s - loss: 1.0687 - accuracy: 0.8571 - val_loss: 1.3726 - val_accuracy: 0.7241\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.72414\n",
      "Epoch 73/2000\n",
      " - 0s - loss: 1.0499 - accuracy: 0.8571 - val_loss: 1.3587 - val_accuracy: 0.6552\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.72414\n",
      "Epoch 74/2000\n",
      " - 0s - loss: 1.0364 - accuracy: 0.8214 - val_loss: 1.3086 - val_accuracy: 0.7241\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.72414\n",
      "Epoch 75/2000\n",
      " - 0s - loss: 1.0007 - accuracy: 0.8571 - val_loss: 1.2762 - val_accuracy: 0.7241\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.72414\n",
      "Epoch 76/2000\n",
      " - 0s - loss: 0.9731 - accuracy: 0.8571 - val_loss: 1.2615 - val_accuracy: 0.7241\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.72414\n",
      "Epoch 77/2000\n",
      " - 0s - loss: 0.9587 - accuracy: 0.8571 - val_loss: 1.2303 - val_accuracy: 0.7241\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.72414\n",
      "Epoch 78/2000\n",
      " - 0s - loss: 0.9360 - accuracy: 0.8571 - val_loss: 1.2033 - val_accuracy: 0.7241\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.72414\n",
      "Epoch 79/2000\n",
      " - 0s - loss: 0.9088 - accuracy: 0.8571 - val_loss: 1.1754 - val_accuracy: 0.7241\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.72414\n",
      "Epoch 80/2000\n",
      " - 0s - loss: 0.8865 - accuracy: 0.8571 - val_loss: 1.1519 - val_accuracy: 0.7241\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.72414\n",
      "Epoch 81/2000\n",
      " - 0s - loss: 0.8716 - accuracy: 0.8571 - val_loss: 1.1342 - val_accuracy: 0.7241\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.72414\n",
      "Epoch 82/2000\n",
      " - 0s - loss: 0.8546 - accuracy: 0.8571 - val_loss: 1.0983 - val_accuracy: 0.7241\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.72414\n",
      "Epoch 83/2000\n",
      " - 0s - loss: 0.8281 - accuracy: 0.8571 - val_loss: 1.0755 - val_accuracy: 0.7241\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.72414\n",
      "Epoch 84/2000\n",
      " - 0s - loss: 0.8088 - accuracy: 0.8571 - val_loss: 1.0639 - val_accuracy: 0.7931\n",
      "\n",
      "Epoch 00084: val_accuracy improved from 0.72414 to 0.79310, saving model to ./model/84-0.8571.hdf5\n",
      "Epoch 85/2000\n",
      " - 0s - loss: 0.7955 - accuracy: 0.8929 - val_loss: 1.0341 - val_accuracy: 0.7241\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.79310\n",
      "Epoch 86/2000\n",
      " - 0s - loss: 0.7751 - accuracy: 0.8571 - val_loss: 1.0107 - val_accuracy: 0.7931\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.79310\n",
      "Epoch 87/2000\n",
      " - 0s - loss: 0.7529 - accuracy: 0.8929 - val_loss: 0.9864 - val_accuracy: 0.7931\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.79310\n",
      "Epoch 88/2000\n",
      " - 0s - loss: 0.7346 - accuracy: 0.8929 - val_loss: 0.9636 - val_accuracy: 0.7931\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.79310\n",
      "Epoch 89/2000\n",
      " - 0s - loss: 0.7204 - accuracy: 0.8929 - val_loss: 0.9510 - val_accuracy: 0.7931\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.79310\n",
      "Epoch 90/2000\n",
      " - 0s - loss: 0.7055 - accuracy: 0.8929 - val_loss: 0.9217 - val_accuracy: 0.7931\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.79310\n",
      "Epoch 91/2000\n",
      " - 0s - loss: 0.6852 - accuracy: 0.8929 - val_loss: 0.9018 - val_accuracy: 0.7931\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.79310\n",
      "Epoch 92/2000\n",
      " - 0s - loss: 0.6675 - accuracy: 0.8929 - val_loss: 0.8863 - val_accuracy: 0.7931\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.79310\n",
      "Epoch 93/2000\n",
      " - 0s - loss: 0.6538 - accuracy: 0.8929 - val_loss: 0.8618 - val_accuracy: 0.7931\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.79310\n",
      "Epoch 94/2000\n",
      " - 0s - loss: 0.6390 - accuracy: 0.8929 - val_loss: 0.8446 - val_accuracy: 0.7931\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.79310\n",
      "Epoch 95/2000\n",
      " - 0s - loss: 0.6223 - accuracy: 0.8929 - val_loss: 0.8220 - val_accuracy: 0.7931\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.79310\n",
      "Epoch 96/2000\n",
      " - 0s - loss: 0.6056 - accuracy: 0.8929 - val_loss: 0.8044 - val_accuracy: 0.7931\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.79310\n",
      "Epoch 97/2000\n",
      " - 0s - loss: 0.5919 - accuracy: 0.8929 - val_loss: 0.7929 - val_accuracy: 0.7931\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.79310\n",
      "Epoch 98/2000\n",
      " - 0s - loss: 0.5790 - accuracy: 0.8929 - val_loss: 0.7706 - val_accuracy: 0.7931\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.79310\n",
      "Epoch 99/2000\n",
      " - 0s - loss: 0.5636 - accuracy: 0.8929 - val_loss: 0.7538 - val_accuracy: 0.7931\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.79310\n",
      "Epoch 100/2000\n",
      " - 0s - loss: 0.5484 - accuracy: 0.8929 - val_loss: 0.7372 - val_accuracy: 0.7931\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.79310\n",
      "Epoch 101/2000\n",
      " - 0s - loss: 0.5351 - accuracy: 0.8929 - val_loss: 0.7187 - val_accuracy: 0.8621\n",
      "\n",
      "Epoch 00101: val_accuracy improved from 0.79310 to 0.86207, saving model to ./model/101-0.8929.hdf5\n",
      "Epoch 102/2000\n",
      " - 0s - loss: 0.5226 - accuracy: 0.9286 - val_loss: 0.7058 - val_accuracy: 0.7931\n",
      "\n",
      "Epoch 00102: val_accuracy did not improve from 0.86207\n",
      "Epoch 103/2000\n",
      " - 0s - loss: 0.5096 - accuracy: 0.8929 - val_loss: 0.6852 - val_accuracy: 0.8621\n",
      "\n",
      "Epoch 00103: val_accuracy did not improve from 0.86207\n",
      "Epoch 104/2000\n",
      " - 0s - loss: 0.4955 - accuracy: 0.9286 - val_loss: 0.6688 - val_accuracy: 0.8621\n",
      "\n",
      "Epoch 00104: val_accuracy did not improve from 0.86207\n",
      "Epoch 105/2000\n",
      " - 0s - loss: 0.4825 - accuracy: 0.9286 - val_loss: 0.6544 - val_accuracy: 0.9310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00105: val_accuracy improved from 0.86207 to 0.93103, saving model to ./model/105-0.9286.hdf5\n",
      "Epoch 106/2000\n",
      " - 0s - loss: 0.4708 - accuracy: 0.9643 - val_loss: 0.6353 - val_accuracy: 0.9310\n",
      "\n",
      "Epoch 00106: val_accuracy did not improve from 0.93103\n",
      "Epoch 107/2000\n",
      " - 0s - loss: 0.4590 - accuracy: 0.9643 - val_loss: 0.6217 - val_accuracy: 0.9310\n",
      "\n",
      "Epoch 00107: val_accuracy did not improve from 0.93103\n",
      "Epoch 108/2000\n",
      " - 0s - loss: 0.4468 - accuracy: 0.9643 - val_loss: 0.6040 - val_accuracy: 0.9310\n",
      "\n",
      "Epoch 00108: val_accuracy did not improve from 0.93103\n",
      "Epoch 109/2000\n",
      " - 0s - loss: 0.4345 - accuracy: 0.9643 - val_loss: 0.5891 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00109: val_accuracy improved from 0.93103 to 1.00000, saving model to ./model/109-0.9643.hdf5\n",
      "Epoch 110/2000\n",
      " - 0s - loss: 0.4233 - accuracy: 1.0000 - val_loss: 0.5772 - val_accuracy: 0.9310\n",
      "\n",
      "Epoch 00110: val_accuracy did not improve from 1.00000\n",
      "Epoch 111/2000\n",
      " - 0s - loss: 0.4125 - accuracy: 0.9643 - val_loss: 0.5603 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00111: val_accuracy did not improve from 1.00000\n",
      "Epoch 112/2000\n",
      " - 0s - loss: 0.4014 - accuracy: 1.0000 - val_loss: 0.5473 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00112: val_accuracy did not improve from 1.00000\n",
      "Epoch 113/2000\n",
      " - 0s - loss: 0.3903 - accuracy: 1.0000 - val_loss: 0.5332 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00113: val_accuracy did not improve from 1.00000\n",
      "Epoch 114/2000\n",
      " - 0s - loss: 0.3798 - accuracy: 1.0000 - val_loss: 0.5184 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00114: val_accuracy did not improve from 1.00000\n",
      "Epoch 115/2000\n",
      " - 0s - loss: 0.3698 - accuracy: 1.0000 - val_loss: 0.5069 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00115: val_accuracy did not improve from 1.00000\n",
      "Epoch 116/2000\n",
      " - 0s - loss: 0.3597 - accuracy: 1.0000 - val_loss: 0.4916 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00116: val_accuracy did not improve from 1.00000\n",
      "Epoch 117/2000\n",
      " - 0s - loss: 0.3496 - accuracy: 1.0000 - val_loss: 0.4783 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00117: val_accuracy did not improve from 1.00000\n",
      "Epoch 118/2000\n",
      " - 0s - loss: 0.3399 - accuracy: 1.0000 - val_loss: 0.4664 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00118: val_accuracy did not improve from 1.00000\n",
      "Epoch 119/2000\n",
      " - 0s - loss: 0.3307 - accuracy: 1.0000 - val_loss: 0.4518 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00119: val_accuracy did not improve from 1.00000\n",
      "Epoch 120/2000\n",
      " - 0s - loss: 0.3214 - accuracy: 1.0000 - val_loss: 0.4399 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00120: val_accuracy did not improve from 1.00000\n",
      "Epoch 121/2000\n",
      " - 0s - loss: 0.3122 - accuracy: 1.0000 - val_loss: 0.4276 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00121: val_accuracy did not improve from 1.00000\n",
      "Epoch 122/2000\n",
      " - 0s - loss: 0.3033 - accuracy: 1.0000 - val_loss: 0.4149 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00122: val_accuracy did not improve from 1.00000\n",
      "Epoch 123/2000\n",
      " - 0s - loss: 0.2948 - accuracy: 1.0000 - val_loss: 0.4042 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00123: val_accuracy did not improve from 1.00000\n",
      "Epoch 124/2000\n",
      " - 0s - loss: 0.2863 - accuracy: 1.0000 - val_loss: 0.3922 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00124: val_accuracy did not improve from 1.00000\n",
      "Epoch 125/2000\n",
      " - 0s - loss: 0.2779 - accuracy: 1.0000 - val_loss: 0.3806 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00125: val_accuracy did not improve from 1.00000\n",
      "Epoch 126/2000\n",
      " - 0s - loss: 0.2699 - accuracy: 1.0000 - val_loss: 0.3705 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00126: val_accuracy did not improve from 1.00000\n",
      "Epoch 127/2000\n",
      " - 0s - loss: 0.2621 - accuracy: 1.0000 - val_loss: 0.3590 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00127: val_accuracy did not improve from 1.00000\n",
      "Epoch 128/2000\n",
      " - 0s - loss: 0.2543 - accuracy: 1.0000 - val_loss: 0.3483 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00128: val_accuracy did not improve from 1.00000\n",
      "Epoch 129/2000\n",
      " - 0s - loss: 0.2468 - accuracy: 1.0000 - val_loss: 0.3386 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00129: val_accuracy did not improve from 1.00000\n",
      "Epoch 130/2000\n",
      " - 0s - loss: 0.2395 - accuracy: 1.0000 - val_loss: 0.3279 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00130: val_accuracy did not improve from 1.00000\n",
      "Epoch 131/2000\n",
      " - 0s - loss: 0.2323 - accuracy: 1.0000 - val_loss: 0.3178 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00131: val_accuracy did not improve from 1.00000\n",
      "Epoch 132/2000\n",
      " - 0s - loss: 0.2253 - accuracy: 1.0000 - val_loss: 0.3086 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00132: val_accuracy did not improve from 1.00000\n",
      "Epoch 133/2000\n",
      " - 0s - loss: 0.2185 - accuracy: 1.0000 - val_loss: 0.2986 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00133: val_accuracy did not improve from 1.00000\n",
      "Epoch 134/2000\n",
      " - 0s - loss: 0.2118 - accuracy: 1.0000 - val_loss: 0.2891 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00134: val_accuracy did not improve from 1.00000\n",
      "Epoch 135/2000\n",
      " - 0s - loss: 0.2053 - accuracy: 1.0000 - val_loss: 0.2806 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00135: val_accuracy did not improve from 1.00000\n",
      "Epoch 136/2000\n",
      " - 0s - loss: 0.1990 - accuracy: 1.0000 - val_loss: 0.2715 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00136: val_accuracy did not improve from 1.00000\n",
      "Epoch 137/2000\n",
      " - 0s - loss: 0.1928 - accuracy: 1.0000 - val_loss: 0.2627 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00137: val_accuracy did not improve from 1.00000\n",
      "Epoch 138/2000\n",
      " - 0s - loss: 0.1869 - accuracy: 1.0000 - val_loss: 0.2547 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00138: val_accuracy did not improve from 1.00000\n",
      "Epoch 139/2000\n",
      " - 0s - loss: 0.1810 - accuracy: 1.0000 - val_loss: 0.2465 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00139: val_accuracy did not improve from 1.00000\n",
      "Epoch 140/2000\n",
      " - 0s - loss: 0.1753 - accuracy: 1.0000 - val_loss: 0.2383 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00140: val_accuracy did not improve from 1.00000\n",
      "Epoch 141/2000\n",
      " - 0s - loss: 0.1698 - accuracy: 1.0000 - val_loss: 0.2309 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00141: val_accuracy did not improve from 1.00000\n",
      "Epoch 142/2000\n",
      " - 0s - loss: 0.1645 - accuracy: 1.0000 - val_loss: 0.2235 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00142: val_accuracy did not improve from 1.00000\n",
      "Epoch 143/2000\n",
      " - 0s - loss: 0.1593 - accuracy: 1.0000 - val_loss: 0.2161 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00143: val_accuracy did not improve from 1.00000\n",
      "Epoch 144/2000\n",
      " - 0s - loss: 0.1543 - accuracy: 1.0000 - val_loss: 0.2091 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00144: val_accuracy did not improve from 1.00000\n",
      "Epoch 145/2000\n",
      " - 0s - loss: 0.1495 - accuracy: 1.0000 - val_loss: 0.2026 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00145: val_accuracy did not improve from 1.00000\n",
      "Epoch 146/2000\n",
      " - 0s - loss: 0.1448 - accuracy: 1.0000 - val_loss: 0.1959 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00146: val_accuracy did not improve from 1.00000\n",
      "Epoch 147/2000\n",
      " - 0s - loss: 0.1402 - accuracy: 1.0000 - val_loss: 0.1894 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00147: val_accuracy did not improve from 1.00000\n",
      "Epoch 148/2000\n",
      " - 0s - loss: 0.1359 - accuracy: 1.0000 - val_loss: 0.1835 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00148: val_accuracy did not improve from 1.00000\n",
      "Epoch 149/2000\n",
      " - 0s - loss: 0.1316 - accuracy: 1.0000 - val_loss: 0.1776 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00149: val_accuracy did not improve from 1.00000\n",
      "Epoch 150/2000\n",
      " - 0s - loss: 0.1276 - accuracy: 1.0000 - val_loss: 0.1717 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00150: val_accuracy did not improve from 1.00000\n",
      "Epoch 151/2000\n",
      " - 0s - loss: 0.1236 - accuracy: 1.0000 - val_loss: 0.1663 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00151: val_accuracy did not improve from 1.00000\n",
      "Epoch 152/2000\n",
      " - 0s - loss: 0.1198 - accuracy: 1.0000 - val_loss: 0.1611 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00152: val_accuracy did not improve from 1.00000\n",
      "Epoch 153/2000\n",
      " - 0s - loss: 0.1162 - accuracy: 1.0000 - val_loss: 0.1560 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00153: val_accuracy did not improve from 1.00000\n",
      "Epoch 154/2000\n",
      " - 0s - loss: 0.1127 - accuracy: 1.0000 - val_loss: 0.1510 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00154: val_accuracy did not improve from 1.00000\n",
      "Epoch 155/2000\n",
      " - 0s - loss: 0.1093 - accuracy: 1.0000 - val_loss: 0.1464 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00155: val_accuracy did not improve from 1.00000\n",
      "Epoch 156/2000\n",
      " - 0s - loss: 0.1060 - accuracy: 1.0000 - val_loss: 0.1419 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00156: val_accuracy did not improve from 1.00000\n",
      "Epoch 157/2000\n",
      " - 0s - loss: 0.1029 - accuracy: 1.0000 - val_loss: 0.1375 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00157: val_accuracy did not improve from 1.00000\n",
      "Epoch 158/2000\n",
      " - 0s - loss: 0.0999 - accuracy: 1.0000 - val_loss: 0.1333 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00158: val_accuracy did not improve from 1.00000\n",
      "Epoch 159/2000\n",
      " - 0s - loss: 0.0970 - accuracy: 1.0000 - val_loss: 0.1293 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00159: val_accuracy did not improve from 1.00000\n",
      "Epoch 160/2000\n",
      " - 0s - loss: 0.0942 - accuracy: 1.0000 - val_loss: 0.1255 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00160: val_accuracy did not improve from 1.00000\n",
      "Epoch 161/2000\n",
      " - 0s - loss: 0.0915 - accuracy: 1.0000 - val_loss: 0.1217 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00161: val_accuracy did not improve from 1.00000\n",
      "Epoch 162/2000\n",
      " - 0s - loss: 0.0889 - accuracy: 1.0000 - val_loss: 0.1181 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00162: val_accuracy did not improve from 1.00000\n",
      "Epoch 163/2000\n",
      " - 0s - loss: 0.0865 - accuracy: 1.0000 - val_loss: 0.1147 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00163: val_accuracy did not improve from 1.00000\n",
      "Epoch 164/2000\n",
      " - 0s - loss: 0.0841 - accuracy: 1.0000 - val_loss: 0.1114 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00164: val_accuracy did not improve from 1.00000\n",
      "Epoch 165/2000\n",
      " - 0s - loss: 0.0818 - accuracy: 1.0000 - val_loss: 0.1082 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00165: val_accuracy did not improve from 1.00000\n",
      "Epoch 166/2000\n",
      " - 0s - loss: 0.0796 - accuracy: 1.0000 - val_loss: 0.1051 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00166: val_accuracy did not improve from 1.00000\n",
      "Epoch 167/2000\n",
      " - 0s - loss: 0.0775 - accuracy: 1.0000 - val_loss: 0.1022 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00167: val_accuracy did not improve from 1.00000\n",
      "Epoch 168/2000\n",
      " - 0s - loss: 0.0754 - accuracy: 1.0000 - val_loss: 0.0994 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00168: val_accuracy did not improve from 1.00000\n",
      "Epoch 169/2000\n",
      " - 0s - loss: 0.0735 - accuracy: 1.0000 - val_loss: 0.0967 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00169: val_accuracy did not improve from 1.00000\n",
      "Epoch 170/2000\n",
      " - 0s - loss: 0.0716 - accuracy: 1.0000 - val_loss: 0.0941 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00170: val_accuracy did not improve from 1.00000\n",
      "Epoch 171/2000\n",
      " - 0s - loss: 0.0698 - accuracy: 1.0000 - val_loss: 0.0916 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00171: val_accuracy did not improve from 1.00000\n",
      "Epoch 172/2000\n",
      " - 0s - loss: 0.0680 - accuracy: 1.0000 - val_loss: 0.0892 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00172: val_accuracy did not improve from 1.00000\n",
      "Epoch 173/2000\n",
      " - 0s - loss: 0.0663 - accuracy: 1.0000 - val_loss: 0.0869 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00173: val_accuracy did not improve from 1.00000\n",
      "Epoch 174/2000\n",
      " - 0s - loss: 0.0647 - accuracy: 1.0000 - val_loss: 0.0847 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00174: val_accuracy did not improve from 1.00000\n",
      "Epoch 175/2000\n",
      " - 0s - loss: 0.0632 - accuracy: 1.0000 - val_loss: 0.0825 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00175: val_accuracy did not improve from 1.00000\n",
      "Epoch 176/2000\n",
      " - 0s - loss: 0.0617 - accuracy: 1.0000 - val_loss: 0.0805 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00176: val_accuracy did not improve from 1.00000\n",
      "Epoch 177/2000\n",
      " - 0s - loss: 0.0602 - accuracy: 1.0000 - val_loss: 0.0785 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00177: val_accuracy did not improve from 1.00000\n",
      "Epoch 178/2000\n",
      " - 0s - loss: 0.0588 - accuracy: 1.0000 - val_loss: 0.0766 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00178: val_accuracy did not improve from 1.00000\n",
      "Epoch 179/2000\n",
      " - 0s - loss: 0.0575 - accuracy: 1.0000 - val_loss: 0.0748 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00179: val_accuracy did not improve from 1.00000\n",
      "Epoch 180/2000\n",
      " - 0s - loss: 0.0562 - accuracy: 1.0000 - val_loss: 0.0730 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00180: val_accuracy did not improve from 1.00000\n",
      "Epoch 181/2000\n",
      " - 0s - loss: 0.0550 - accuracy: 1.0000 - val_loss: 0.0714 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00181: val_accuracy did not improve from 1.00000\n",
      "Epoch 182/2000\n",
      " - 0s - loss: 0.0538 - accuracy: 1.0000 - val_loss: 0.0697 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00182: val_accuracy did not improve from 1.00000\n",
      "Epoch 183/2000\n",
      " - 0s - loss: 0.0526 - accuracy: 1.0000 - val_loss: 0.0681 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00183: val_accuracy did not improve from 1.00000\n",
      "Epoch 184/2000\n",
      " - 0s - loss: 0.0515 - accuracy: 1.0000 - val_loss: 0.0666 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00184: val_accuracy did not improve from 1.00000\n",
      "Epoch 185/2000\n",
      " - 0s - loss: 0.0504 - accuracy: 1.0000 - val_loss: 0.0652 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00185: val_accuracy did not improve from 1.00000\n",
      "Epoch 186/2000\n",
      " - 0s - loss: 0.0494 - accuracy: 1.0000 - val_loss: 0.0637 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00186: val_accuracy did not improve from 1.00000\n",
      "Epoch 187/2000\n",
      " - 0s - loss: 0.0484 - accuracy: 1.0000 - val_loss: 0.0624 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00187: val_accuracy did not improve from 1.00000\n",
      "Epoch 188/2000\n",
      " - 0s - loss: 0.0474 - accuracy: 1.0000 - val_loss: 0.0611 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00188: val_accuracy did not improve from 1.00000\n",
      "Epoch 189/2000\n",
      " - 0s - loss: 0.0465 - accuracy: 1.0000 - val_loss: 0.0598 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00189: val_accuracy did not improve from 1.00000\n",
      "Epoch 190/2000\n",
      " - 0s - loss: 0.0456 - accuracy: 1.0000 - val_loss: 0.0586 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00190: val_accuracy did not improve from 1.00000\n",
      "Epoch 191/2000\n",
      " - 0s - loss: 0.0447 - accuracy: 1.0000 - val_loss: 0.0574 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00191: val_accuracy did not improve from 1.00000\n",
      "Epoch 192/2000\n",
      " - 0s - loss: 0.0438 - accuracy: 1.0000 - val_loss: 0.0562 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00192: val_accuracy did not improve from 1.00000\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, Dense, LSTM\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=64, input_length=max_len-1)) \n",
    "\n",
    "# 여기부터 코드를 수정해서 레이어를 정의하고 학습시키세요\n",
    "\n",
    "model.add(LSTM(128 ,activation='tanh', recurrent_activation='sigmoid')) # 3\n",
    "model.add(Dense(vocab_size, activation='softmax')) # 4\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # 5\n",
    "\n",
    "hist = model.fit(X, y, validation_split=0.5, epochs=2000,verbose=2, callbacks=[checkpoint,early_stopping]) # 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델 학습 과정 표시하기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_accuracy'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(model, t, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
    "    init_word = current_word # 처음 들어온 단어도 마지막에 같이 출력하기위해 저장\n",
    "    sentence = ''\n",
    "    for _ in range(n): # n번 반복\n",
    "        encoded = t.texts_to_sequences([current_word])[0] # 현재 단어에 대한 정수 인코딩\n",
    "        encoded = pad_sequences([encoded], maxlen=max_len-1, padding='pre') # 데이터에 대한 패딩\n",
    "        result = model.predict_classes(encoded, verbose=0)\n",
    "    # 입력한 X(현재 단어)에 대해서 Y를 예측하고 Y(예측한 단어)를 result에 저장.\n",
    "        for word, index in t.word_index.items(): \n",
    "            if index == result: # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면\n",
    "                break # 해당 단어가 예측 단어이므로 break\n",
    "        current_word = current_word + ' '  + word # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
    "        sentence = sentence + ' ' + word # 예측 단어를 문장에 저장\n",
    "    # for문이므로 이 행동을 다시 반복\n",
    "    sentence = init_word + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주임교수님 교수님들에게 의견을\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, t, '주임교수님', 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대우관 주 화요일에\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, t, '대우관', 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "의견 교수님들에게 의견을\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, t, '의견', 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
